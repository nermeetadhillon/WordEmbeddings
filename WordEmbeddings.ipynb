{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa2c763f9b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Prepare data\n",
    "**3 points**\n",
    "\n",
    "In this task, you should prepare your data, which is a list of tuples. Each tuple has two elements: a list of context words, and the target word.\n",
    "Here both context words and target word are represented by word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3  # Define the context size. Default value 3, which means the context includes 3 words to the left, 3 to the right\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = list(vocab)\n",
    "word_indices = [word_to_idx[w] for w in raw_text]\n",
    "\n",
    "def prepare_data(word_indices):\n",
    "    data = []\n",
    "    for i in range(CONTEXT_SIZE, len(word_indices) - CONTEXT_SIZE):\n",
    "\n",
    "        #### START YOUR CODE ####\n",
    "        # Hint: You can intialize context to an empty list\n",
    "        # and then use a for loop to append elements to context propoerly.\n",
    "        context = [word_indices[i-3],word_indices[i-2],word_indices[i-1],\n",
    "                  word_indices[i+1],word_indices[i+2],word_indices[i+3]\n",
    "                  ] # added\n",
    "        target =  word_indices[i] # added\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "        data.append((context, target))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Task 1. Do not change the code below.\n",
    "data = prepare_data(word_indices)\n",
    "print('data[0]:', data[0])\n",
    "ctx, tgt = data[0]\n",
    "print('context words:', [idx_to_word[c] for c in ctx])\n",
    "print('target word:', idx_to_word[tgt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected output\n",
    "\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|data\\[0\\]: |(\\[1, 20, 39, 37, 45, 35\\], 3)|\n",
    "|context words: |\\['We', 'are', 'about', 'study', 'the', 'idea'\\]|\n",
    "|target word: | to|\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement a CBOW model\n",
    "\n",
    "**4 points**\n",
    "\n",
    "In this task, you will implement a CBOW model. In the `__init__()` method, define the size of `self.embeddings` and `self.linear` properly.\n",
    "\n",
    "The `self.linear` takes the average embeddings of all context words as input, and the output size is `vocab_size`.\n",
    "It is followed by a softmax activation (`nn.LogSoftmax`).\n",
    "\n",
    "The `forward()` method has a input argument `inputs`, which is the context word indices (in a `torch.long` tensor).\n",
    "You should get the embeddings of all context words, and compute the average emebdding (into the `embeds` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #### START YOUR CODE ####\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) #added\n",
    "        self.linear = nn.Linear(embedding_dim, 10) #added,128: output features\n",
    "        #### END YOUR CODE ####\n",
    "        \n",
    "        self.act = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #### START YOUR CODE ####\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1) # added\n",
    "        #### END YOUR CODE ####\n",
    "        \n",
    "        out = self.linear(embeds)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Task 2. Do not change the code blow\n",
    "torch.manual_seed(0)\n",
    "\n",
    "m = CBOW(10, 20)\n",
    "test_input = torch.tensor([1,2,3], dtype=torch.long)\n",
    "test_output = m(test_input)\n",
    "\n",
    "print('test_output.shape', test_output.shape)\n",
    "print('test_output', test_output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|test_output.shape| torch.Size(\\[1, 10\\])|\n",
    "|test_output|tensor(\\[\\[-1.6878, -4.2108, -5.0252, -2.9802, -3.1362, -1.5436, -1.4120, -3.2485, -1.6490, -4.5009\\]\\])|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Training loop\n",
    "**2 points**\n",
    "\n",
    "In this task, you will complete the training loop. \n",
    "\n",
    "You should create `ctx_tensor` and `tgt_tensor` out of `ctx` and `tgt`, respectively. *Hint*: you need to put `tgt` to a list before craeting the `tgt_tensor`, so that the resulting tensor is of the correct dimension that is acceptable to `nn.NLLLoss()`.\n",
    "\n",
    "`ctx_tensor` is used to compute `output`. `loss_function()` is called upon `output` and `tgt_tensor` to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "EMDEDDING_DIM = 100\n",
    "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "\n",
    "    for ctx, tgt in data:\n",
    "        #### START YOUR CODE ####\n",
    "        ctx_tensor = torch.tensor(ctx, dtype=torch.long) #added ,torch.long: for long typre\n",
    "        tgt_tensor = torch.tensor(tgt, dtype=torch.long) #added\n",
    "        output = model(ctx_tensor) #added\n",
    "\n",
    "        # The try...except code is to help you debug. You can leave them unchanged. \n",
    "        try:\n",
    "            total_loss += loss_function(output,tgt_tensor) #added\n",
    "        except Exception:\n",
    "            print(ctx_tensor)\n",
    "            print(tgt_tensor)\n",
    "            raise\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "    #optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print training information\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        print(f'Loss within epoch {epoch}: ', total_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "\n",
    "You should obeserve the loss decreasing from 100+ (at epoch 5) to around 3.x (at epoch 95).\n",
    "The absolute values do not matter.\n",
    "\n",
    "<!-- |&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|Loss within epoch 5: | 138.33709716796875|\n",
    "|Loss within epoch 10: | 70.50218963623047|\n",
    "|Loss within epoch 15: | 38.877227783203125|\n",
    "|Loss within epoch 20: | 25.064970016479492|\n",
    "|Loss within epoch 25: | 18.110904693603516|\n",
    "|Loss within epoch 30: | 14.05634880065918|\n",
    "|Loss within epoch 35: | 11.44089126586914|\n",
    "|Loss within epoch 40: | 9.62782096862793|\n",
    "|Loss within epoch 45: | 8.302525520324707|\n",
    "|Loss within epoch 50: | 7.293947219848633|\n",
    "|Loss within epoch 55: | 6.501856327056885|\n",
    "|Loss within epoch 60: | 5.863919734954834|\n",
    "|Loss within epoch 65: | 5.339460372924805|\n",
    "|Loss within epoch 70: | 4.900869846343994|\n",
    "|Loss within epoch 75: | 4.528764247894287|\n",
    "|Loss within epoch 80: | 4.209164619445801|\n",
    "|Loss within epoch 85: | 3.9317333698272705|\n",
    "|Loss within epoch 90: | 3.688669443130493|\n",
    "|Loss within epoch 95: | 3.473975896835327| -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "**1 point**\n",
    "\n",
    "In this final task, you will need to find the maximum index among the model output. *Hint*: use `torch.argmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_word(model_output, idx_to_word):\n",
    "    #### START YOUR CODE ####\n",
    "    idx = torch.argmax(model_output[0]).item()\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return idx_to_word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Task 4. Do not change the code blow\n",
    "ctx_words = 'evolution of a is directed by'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model(ctx_tensor)\n",
    "pred = get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|The predicted word is: |\"process\"|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulation!\n",
    "Congratulations! You have now understood how to use word embeddings for some basic NLP tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
